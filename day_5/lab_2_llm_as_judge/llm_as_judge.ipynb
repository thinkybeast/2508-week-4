{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "314a46f5",
   "metadata": {},
   "source": [
    "# LLM as Judge\n",
    "\n",
    "In this lab, youâ€™ll practice evaluating AI outputs by building a judge LLM. Youâ€™ll be given a dataset of student code and AI responses. Some AI outputs give helpful hints, while others provide full solutions. Your goal is to:\n",
    "\n",
    "1. Implement a judge that classifies outputs as `error_present`: `true` or `false`.\n",
    "2. Measure the judgeâ€™s **true positive**, **false positive**, **true negative**, and **false negative** rates.\n",
    "3. Iterate on your prompt to improve judge performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e021ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f2857",
   "metadata": {},
   "source": [
    "## Step 1: Explore the dataset\n",
    "\n",
    "Below we print out our dataset. Familiarize yourself with the types of questions we're working with. We have a problem where our chatbot is too eager to provide solution code when a student asks just for a hint or suggestions for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb793be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('dataset.json', 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "print(json.dumps(dataset, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775dae22",
   "metadata": {},
   "source": [
    "## Step 2: Build a basic LLM as Judge\n",
    "\n",
    "Implement a function that takes an input and an AI output and returns an object like this:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"error_present\": true | false,\n",
    "    \"reasoning\": \"The AI's reasoning behind deciding if the output has the error\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fa61dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_judge(input, output):\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    pass\n",
    "\n",
    "# Example usasge:\n",
    "print(llm_judge(dataset[0]['input'], dataset[0]['output']))\n",
    "# {\n",
    "#     \"error_present\": true,\n",
    "#     \"reasoning\": \"The student requested a hint for a recursive Fibonacci function, but the AI provided a full solution.\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf1235",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate your judge\n",
    "\n",
    "1. Run your judge against the entire dataset. \n",
    "2. Compare your judge's output to `error_present` in the dataset.\n",
    "3. Calculate the following metrics:\n",
    "    - **True Positive (TP).** The AI output contains the error, and the judge correctly flagged it as having the error.\n",
    "    - **False Positive (FP).** The AI output does not contain the error, but the judge incorrectly flagged it as having the error.\n",
    "    - **True Negative (TN).** The AI output does not contain the error, and the judge correctly flagged it as not having the error.\n",
    "    - **False Negative (FN).** The AI output contains the error, but the judge incorrectly flagged it as not having the error.\n",
    "\n",
    "> ðŸ’¡ True/False (T/F): Whether the judge correctly indicated if the error was present.\n",
    ">     - True: Judgeâ€™s prediction matches reality\n",
    ">     - False: Judgeâ€™s prediction does not match reality\n",
    "\n",
    "> ðŸ’¡ Positive/Negative (P/N): Whether the AI output actually contains the error.\n",
    ">     - Positive: AI output has the error (provides solution rather than hint/guidance)\n",
    ">     - Negative: AI output does not have the error (hint or guidance only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9de1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the performance of you LLM as Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05afed5f",
   "metadata": {},
   "source": [
    "## Step 4: Iterate your judge prompt\n",
    "\n",
    "Modify your prompt to reduce false positives or false negatives.\n",
    "\n",
    "Try including examples in your prompt (few-shot) to improve classification.\n",
    "\n",
    "Test again and record metrics.\n",
    "\n",
    "Optional: For more advanced experimentation, set aside a holdout subset of examples (~40%) to test your final judge on unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
