{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6wVZROPMl2P"
      },
      "source": [
        "# API Key & Model Selection\n",
        "\n",
        "The following cell lets you test that you're API key is accessible. To add your API key, follow instructions in `README`.\n",
        "\n",
        "> ðŸ’¡ Remember! This cell needs to be ran before you can run any other cells that utilize the `API_KEY` variable. If you reset the runtime, you'll need to run this cell again.\n",
        "\n",
        "We'll also create a constant to choose our model here so don't have to repeat it throughout every cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qhvm_n_I2yv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()  # Load environment variables from .env file\n",
        "API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "print(API_KEY)\n",
        "\n",
        "MODEL = 'gpt-4o-mini'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-JsNSyDOCVi"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "This code snippet is from the [Text generation and prompting](https://platform.openai.com/docs/guides/text?api-mode=responses) API reference of the OpenAI documentation. Try it out to make sure everything is working so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuNjJ7hnOJhe"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=API_KEY)\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=MODEL,\n",
        "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa5Mw4mMOwgw"
      },
      "source": [
        "# Part 1: Create a Chat Loop\n",
        "\n",
        "Use the [Text generation and prompting](https://platform.openai.com/docs/guides/text?api-mode=responses) guide to complete the following todos.\n",
        "\n",
        "> ðŸ’¡ If you navigate to the documentation yourself, make sure that you're using the \"Responses\" API, not \"Chat Completions\". Both work, but \"Responses\" is newer, and it's what we'll use in examples and solutions.\n",
        "\n",
        "<details>\n",
        "<summary> ðŸ‘‰ Hints! </summary>\n",
        "\n",
        "- You'll need to study the examples to see how to include multiple messages, or \"conversation state\".\n",
        "- The `input` function works in Jupyter Notebooks just like it does in a `.py` file to get input from the user.\n",
        "- Your `chat` function will need to run in a loop. Make sure to provide a way for the user to exit the loop.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doEkoleWPFKU"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(api_key=API_KEY)\n",
        "\n",
        "def chat():\n",
        "\n",
        "    # TODO: Implement the chat loop that:\n",
        "    # 1. Prompts the user for input\n",
        "    # 2. Sends the input to OpenAI\n",
        "    # 3. Displays the response\n",
        "    # 4. Maintains the conversation history\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YukGT6iUDOs"
      },
      "source": [
        "# Part 2: Add Classes to Your Program\n",
        "\n",
        "Managing the messages and history of our conversation is a bit clumsy. Create some classes with whatever methods necessary to make our code more declarative. Use these suggestions or take your own approach:\n",
        "\n",
        "- `Message`: Have a role and content. Takes the form of a `dict` object for API calls.\n",
        "- `ChatHistory`: Maintains an array of `Message` objects. Takes the form of a `list` for API calls.\n",
        "- `ChatManager`: Orchestrates the interaction utilizing `Message` and `ChatHistory` classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcMgsFuSWAZ5"
      },
      "outputs": [],
      "source": [
        "class Message:\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "class ChatHistory:\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "class ChatManager:\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "# TODO: Update how we start a chat to our new OO approach\n",
        "chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EpRBwY1YnNa"
      },
      "source": [
        "# Part 3: Token Management\n",
        "\n",
        "When making API calls, we're charged per-token. Let's make sure that our user can't surpass a certain number of tokens in their message.\n",
        "\n",
        "First, skim this information on [Managing Tokens](https://platform.openai.com/docs/advanced-usage#managing-tokens). Towards the end of this section, they mention the Python library [tiktoken](https://github.com/openai/tiktoken).\n",
        "\n",
        "Using the documentation for tiktoken (the README file of the tiktoken GitHub repo), and the [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb), implement a feature that stops conversations from exceeding a certain token limit. Make sure to test that the result you're getting from `tiktoken` is close to the actual tokens being used, which you can find in the API response.\n",
        "\n",
        "> ðŸ’¡ You'll need to use `poetry add tiktoken` to update your project with the new library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c6VhgGfaHZd"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "class TokenManager:\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "# TODO: Make any necessary updates to ChatManager, Message, and ChatHistory to incorporate token management\n",
        "\n",
        "SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
        "chat = ChatManager(MODEL, SYSTEM_PROMPT)\n",
        "chat.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9zd3Aluh3DJ"
      },
      "source": [
        "# Bonus Ideas\n",
        "\n",
        "If you have extra time, here are some ideas:\n",
        "\n",
        "- **Customize the system prompt**. Allow the user to either select some attributes, like 'formal' vs 'informal', or let them set their own system prompt.\n",
        "\n",
        "- **Save past conversations to a file.** Create a directory that stores conversations once they've ended, along with meta-data like the date and time of the conversation.\n",
        "\n",
        "- **Implement a 'help' command.** Give the user a way to access a help menu with more information. You could allow for other commands, like checking token limits.\n",
        "\n",
        "- **Timeout the Chatbot**. After a certain amount of inactivity, automatically end the conversation.\n",
        "\n",
        "- **Error Handling**. We don't have any error handling right now. Look into possible responses from the OpenAI API and handle them gracefully. You can view information about [the response object here](https://platform.openai.com/docs/api-reference/responses/object)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
