{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tOhJBQMr9nv"
      },
      "source": [
        "# Lab 1: Comparing Embeddings\n",
        "\n",
        "In this lab, you'll explore word embeddings for the first time. You'll investigate word similarities, analogies, and vector operations using travel-related concepts. In a larger project, you would typically generate these embeddings yourself using a tool like Word2Vec. For simplicity, we've provided pre-trained vectors. In the next lab, you'll learn how to generate embeddings from your own dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie_linT5tZ6L"
      },
      "source": [
        "## Reading our Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4Qz8hrB9Ghr"
      },
      "outputs": [],
      "source": [
        "with open('vectors.txt', 'r') as f:\n",
        "    vectors = f.read()\n",
        "    print(vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIq93lSFKQhQ"
      },
      "source": [
        "## A Caveat about our Embeddings\n",
        "\n",
        "Normally, word embeddings involve tens or hundreds of thousands of vectors, each with 50 to 300 dimensions or more. For this exercise, we've filtered a small subset of travel-related embeddings with relatively low dimensionality (50). This setup is great for learning and prototyping. However, in real-world applications, you'd typically work with much larger datasets—both in terms of vocabulary size and vector dimensions—which requires significantly more compute power and memory.\n",
        "\n",
        "## Loading Embeddings\n",
        "\n",
        "Let's load our embeddings into a Python dictionary. The key will be the word, and the value will be its corresponding embedding. For the values, we'll use a NumPy array, which allows us to efficiently perform vector math functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G07JSGDJ9Zs1"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "\n",
        "# Function to load our simple vector file\n",
        "def load_word_vectors(file_path):\n",
        "    word_vectors = {}\n",
        "    with open('vectors.txt') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = numpy.array([float(val) for val in values[1:]])\n",
        "            word_vectors[word] = vector\n",
        "    return word_vectors\n",
        "\n",
        "word_vectors = load_word_vectors('./vectors.txt')\n",
        "\n",
        "print('Vector for \"paris\":')\n",
        "print(word_vectors['paris'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNQW7THK9mes"
      },
      "source": [
        "# Part 1: Defining Cosine Similarity\n",
        "\n",
        "First, we need to define a cosine similarity function. Remember:\n",
        "\n",
        "- a similarity of 1 means the vectors are identical in direction\n",
        "- a similarity of 0 means they are orthogonal (Not similar, not opposites)\n",
        "- a similarity of -1 means they are opposites\n",
        "\n",
        "Cosine Similarity Forumula:\n",
        "\n",
        "`(A, B) = (A · B) / (||A|| * ||B||)`\n",
        "\n",
        "Where:\n",
        "\n",
        "`A · B`: is the dot product of vectors A and B.\n",
        "\n",
        "`||A||` and `||B||` are the magnitudes (Euclidean norms) of vectors A and B.\n",
        "\n",
        "We've provided the expected cosine similarity for two pairs, so you'll know when you've done it correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkyl_-Ao_BAk"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "# Test our cosine_similarity\n",
        "print('Similarity between vectors for \"palace\" and \"temple\":')\n",
        "print(cosine_similarity(word_vectors['palace'], word_vectors['temple'])) # 0.7437538473269875\n",
        "print()\n",
        "print('Similarity between vectors for \"dock\" and \"italy\":')\n",
        "print(cosine_similarity(word_vectors['dock'], word_vectors['italy']))    # 0.055356290110446675"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2Z7XIRM9d9P"
      },
      "source": [
        "\n",
        "\n",
        "## Part 2: Exploring Cosine Similarity\n",
        "\n",
        "1. Create a few more tests with words from our set. Here's the list of words we have embeddings for:\n",
        "\n",
        "'paris', 'france', 'rome', 'italy', 'tokyo', 'japan', 'barcelona', 'spain', 'london', 'england', 'berlin', 'germany', 'amsterdam', 'netherlands', 'vienna', 'austria', 'lisbon', 'portugal', 'athens', 'greece', 'hotel', 'hostel', 'resort', 'airbnb', 'motel', 'apartment', 'guesthouse', 'cabin', 'flight', 'train', 'bus', 'taxi', 'subway', 'ferry', 'cruise', 'bicycle', 'beach', 'mountain', 'desert', 'jungle', 'island', 'lake', 'river', 'forest', 'museum', 'restaurant', 'cafe', 'bar', 'market', 'mall', 'temple', 'church', 'mosque', 'castle', 'palace', 'airport', 'station', 'terminal', 'port', 'dock', 'passport', 'visa', 'ticket', 'boarding', 'customs', 'immigration', 'hiking', 'swimming', 'surfing', 'skiing', 'diving', 'camping', 'climbing', 'kayaking', 'sightseeing', 'shopping', 'dining', 'photography', 'touring', 'relaxing', 'backpack', 'suitcase', 'luggage', 'camera', 'map', 'guidebook', 'sunscreen', 'umbrella', 'wallet', 'charger', 'adapter', 'pillow'\n",
        "\n",
        "2. Make note of any pairs that surprise you. What similarity score did you get vs what you expected? Can you explain the difference?\n",
        "\n",
        "3. Using your intuition, try to find a pair that gives us a **negative** similarity score. Could you find one? Why do you think it's difficult to find a negative similarity score, given this set of words?\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDWa6w-8LEQ-"
      },
      "outputs": [],
      "source": [
        "# TODO: Compare similarity scores for several pairs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUKYI0vDCGDF"
      },
      "source": [
        "## Part 3: Define `find_nearest_word`\n",
        "\n",
        "Write a function, `find_nearest_word`, that takes the following parameters:\n",
        "\n",
        "- `target_vector`: A NumPy array representing the target vector.\n",
        "\n",
        "- `word_vectors`: A dictionary where the keys are words (strings), and the values are the corresponding word vectors (NumPy arrays).\n",
        "\n",
        "- `exclude` (optional): A list of words to exclude from the search. The default value should be an empty list.\n",
        "\n",
        "The function should return a tuple: the word with the highest cosine similarity to the target vector (excluding any words in the `exclude` list), and its corresponding similarity score.\n",
        "\n",
        "Write some tests that showcase your function and print out the results.\n",
        "\n",
        "Hint: Use the `cosine_similarity` function that was defined in the previous code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpxYKydtsT_3"
      },
      "outputs": [],
      "source": [
        "# Function to find the nearest word to a vector\n",
        "def find_nearest_word(target_vector, word_vectors, exclude=[]):\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    pass\n",
        "\n",
        "print('--- Testing `find_nearest_word` ---\\n')\n",
        "\n",
        "# TODO: Tests showing functionality of `find_nearest_word`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPF8wRcOuYYr"
      },
      "source": [
        "## Part 4: Define `find_nearest_words`\n",
        "\n",
        "Finding the nearest word is interesting, but it would be interesting to see more than just the nearest. Write a function, `find_nearest_words` that does just that.\n",
        "\n",
        "Parameters:\n",
        "\n",
        "- `target_vector`: A NumPy array representing the target vector.\n",
        "\n",
        "- `word_vectors`: A dictionary where the keys are words (strings), and the values are the corresponding word vectors (NumPy arrays).\n",
        "\n",
        "- `exclude` (optional): A list of words to exclude from the search. The default value should be an empty list.\n",
        "\n",
        "- `top_n` (optional): An integer representing the number of nearest words we'd like to collect. Default to top 3.\n",
        "\n",
        "The function should return a list of tuples containing: the word with the highest cosine similarity to the target vector (excluding any words in the `exclude` list), and its corresponding similarity score. The list should be sorted from highest similarity score to lowest.\n",
        "\n",
        "Write some tests that showcase your function and print out the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fxozapcHpxS"
      },
      "outputs": [],
      "source": [
        "def find_nearest_words(target_vector, word_vectors, exclude=[], top_n=3):\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "print('--- Testing `find_nearest_words` ---\\n')\n",
        "\n",
        "# TODO: Demonstrate `find_nearest_words`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9FFsnLbxG9w"
      },
      "source": [
        "# Part 5: Define `find_farthest_words`\n",
        "\n",
        "For fun, let's write a function to see which words are furthest from a given word. You don't need to worry about keeping your code DRY, our goal is to explore our embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEwgV1BMxGij"
      },
      "outputs": [],
      "source": [
        "def find_farthest_words(target_vector, word_vectors, exclude=[], last_n=3):\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "print('--- Testing `find_farthest_words` ---\\n')\n",
        "\n",
        "# TODO: Demonstrate `find_farthest_words`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhgnwcr1zdSW"
      },
      "source": [
        "## Part 6: Vector Math\n",
        "\n",
        "Remember this example that demonstrated relations between vectors?\n",
        "\n",
        "king - man + woman = queen\n",
        "\n",
        "The idea is that if we take the vector for \"King\", subtract the vector for \"man\", add the vector for \"woman\", we'll end up nearish to \"queen\"\n",
        "\n",
        "Let's try that out with our sample set. We don't have kings and queens in our travel-related words, so let's try with countries and capitals.\n",
        "\n",
        "Let's see if \"paris\" - \"france\" + \"portugal\" = \"lisbon\".\n",
        "\n",
        "Write some code to calculate the vector resulting from the above calculation, and then print out the 3 nearest words to that vector.\n",
        "\n",
        "Afterwards, come up with your own analogies and see if you get the expected results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpxN0Jxnw89b"
      },
      "outputs": [],
      "source": [
        "# Paris - France + Portugal = Lisbon\n",
        "\n",
        "# TODO: Find 3 nearest words to \"Paris - France + Portugal\"\n",
        "\n",
        "\n",
        "# TODO: Try to demonstrate at least one more analogy\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
