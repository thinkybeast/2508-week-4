{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry8LeHkIaaag"
      },
      "source": [
        "# Semantic Chunking\n",
        "\n",
        "In this lab, we'll demonstrate how to use LangChain's Semantic Chunker to break apart our text into relevant chunks. Before we explore semantic chunking, let's try to convince ourselves that it's worth the effort by exploring some more rudimentary chunking techniques.\n",
        "\n",
        "First, let's look at a sample document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "E4MjgInvYGKB"
      },
      "outputs": [],
      "source": [
        "with open(\"launch_school_docs.txt\") as f:\n",
        "    launch_school_docs = f.read()\n",
        "\n",
        "print(launch_school_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt0ckHNZbPkN"
      },
      "source": [
        "Our text has three basic parts:\n",
        "\n",
        "1. ESLint Installation Instructions\n",
        "2. A Capstone FAQ\n",
        "3. A Forum post for a Launch School Women's Group event.\n",
        "\n",
        "In the realm of data, this is actually quite tidy. Sure, it's fabricated and more messy than actual Launch School data, but it does have good formatting. Let's come up with a few rudimentary strategies we might use and see how our chunks come out.\n",
        "\n",
        "## Strategy 1: Using New-line Characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0T6mYUMlbrWg"
      },
      "outputs": [],
      "source": [
        "# Chunking Based on Newline Characters\n",
        "\n",
        "import textwrap\n",
        "\n",
        "chunks = launch_school_docs.split(\"\\n\")\n",
        "print(*chunks, sep=\"\\n-----------\\n\")\n",
        "\n",
        "print('**************** Chunk data ****************')\n",
        "\n",
        "average_chunk_size = sum([len(chunk) for chunk in chunks]) / len(chunks)\n",
        "print(f\"\\nAverage chunk size: {average_chunk_size}\")\n",
        "\n",
        "smallest_chunk = min(chunks, key=len)\n",
        "print(f\"\\nSmallest chunk: {smallest_chunk}\")\n",
        "print(f\"\\nSmallest chunk length: {len(smallest_chunk)}\")\n",
        "\n",
        "chunks = [chunk for chunk in chunks if len(chunk.strip()) > 0]\n",
        "\n",
        "print(f\"\\nSmallest non-empty chunk: {min(chunks, key=len)}\")\n",
        "print(f\"\\nSmallest non-empty chunk length: {len(min(chunks, key=len))}\")\n",
        "\n",
        "largest_chunk = max(chunks, key=len)\n",
        "print(f\"\\nLargest chunk: \\n{textwrap.fill(largest_chunk, width=100)}\")\n",
        "print(f\"\\nLargest chunk length: {len(largest_chunk)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqHIotkec3Yt"
      },
      "source": [
        "This hasn't worked very well. It would probably give us some relevant chunks, but we have a few problems.\n",
        "\n",
        "1. The lines of code have lost all context. Stand-alone, they don't mean much. Same story with the questions and their answers from the FAQ.\n",
        "2. The chunk sizes vary quite a bit. We can easily filter out empty chunks, but even then the smallest non-empty chunk is just the character \"}\", while the largest has 753 characters. That’s still reasonable, but it’s a wide range. And if our text contained documents without any newline characters, we could easily end up with very large chunks.\n",
        "\n",
        "## Strategy 2: Splitting Into Even-length Chunks\n",
        "\n",
        "Let's try a different technique that should solve some of our problems. Since we're using a small document for demonstration, we'll use smaller chunks than is typical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gfCh6-qf1eH"
      },
      "outputs": [],
      "source": [
        "chunks = [launch_school_docs[i:i+300] for i in range(0, len(launch_school_docs), 300)]\n",
        "\n",
        "print(*chunks, sep=\"\\n-----------\\n\")\n",
        "\n",
        "print('**************** Chunk data ****************')\n",
        "\n",
        "average_chunk_size = sum([len(chunk) for chunk in chunks]) / len(chunks)\n",
        "print(f\"\\nAverage chunk size: {average_chunk_size}\")\n",
        "\n",
        "smallest_chunk = min(chunks, key=len)\n",
        "print(f\"\\nSmallest chunk: {smallest_chunk}\")\n",
        "print(f\"\\nSmallest chunk length: {len(smallest_chunk)}\")\n",
        "\n",
        "largest_chunk = max(chunks, key=len)\n",
        "print(f\"\\nLargest chunk: \\n{textwrap.fill(largest_chunk, width=100)}\")\n",
        "print(f\"\\nLargest chunk length: {len(largest_chunk)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyRBqD-LhYUf"
      },
      "source": [
        "This seems a bit better! At least our code instructions have some context, and we don't have single-letter meaningless chunks like \"}\".\n",
        "\n",
        "That said, we have an issue of chunks existing between two different topics, like this one:\n",
        "\n",
        "> amentals\n",
        "> Databases (nosql, rdbms) & Database Design\n",
        "> Full-stack Development and Frameworks\n",
        "> Cloud Infrastructure\n",
        "> Agile Team-based Development\n",
        "> Software Architecture & System Design\n",
        "> Distributed Systems\n",
        "> Service Oriented Architectures\n",
        "> \n",
        "> What if I don’t reside in the US?\n",
        "> While we have a strong preference fo\n",
        "\n",
        "We also cut sentences themselves in half. Though this is less of a concern as we could easily implement a bit more logic to make the splits at sentences.\n",
        "\n",
        "The biggest problem we need to solve is how to create chunks based on **semantic meaning.**\n",
        "\n",
        "## Thought Challenge: Chunking Semantically\n",
        "\n",
        "Try to think of how we might be able to break our document into chunks based on semantic meaning, given the tools we already know about. Brainstorm a few ideas.\n",
        "\n",
        "<details>\n",
        "<summary>💡 Hint 1 💡</summary>\n",
        "\n",
        "We don't want chunks to split mid-sentence, so as a starting point, imagine we have a list of sentences that make up a document. How can we determine if two sentences should be grouped in the same chunk, or be split into separate chunks?\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>💡 Hint 2 💡</summary>\n",
        "\n",
        "We could embed sentences to give a numerical representation of their \"similarity\" relative to one another.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>💡 Hint 3 💡</summary>\n",
        "\n",
        "You'll need a `cosine_similarity` and `generate_embedding` function. You can snag these from previous labs.\n",
        "\n",
        "</details>\n",
        "\n",
        "Once you've given it some thought, go ahead and follow the steps outlined below, or implement your own idea!\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>📝 Steps to Chunking 📝</summary>\n",
        "\n",
        "Your task is to implement our own chunking strategy. Here's what you need to do:\n",
        "\n",
        "1. Split the text into sentences. The order should be preserved. Even if two sentences are very similar, if they're in different sections, they shouldn't be chunked together.\n",
        "\n",
        "2. Iterate over the sentences, building chunks as you go. To create your chunks:\n",
        "- Embed two sentences.\n",
        "- Calculate a similarity score.\n",
        "- If the score is above a certain threshold, add it to the current chunk.\n",
        "- If the score is below a certain threshold, start a new chunk and add it to this new chunk.\n",
        "\n",
        "3. Return the chunks\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9MgTmBUo7EZ"
      },
      "outputs": [],
      "source": [
        "def semantic_chunking(text, similarity_threshold=0.5):\n",
        "\n",
        "  # TODO\n",
        "\n",
        "  pass\n",
        "\n",
        "\n",
        "chunks = semantic_chunking(launch_school_docs)\n",
        "\n",
        "print(*chunks, sep=\"\\n-----------\\n\")\n",
        "\n",
        "print('**************** Chunk data ****************')\n",
        "\n",
        "print(f\"\\nNumber of chunks: {len(chunks)}\")\n",
        "\n",
        "average_chunk_size = sum([len(chunk) for chunk in chunks]) / len(chunks)\n",
        "print(f\"\\nAverage chunk size: {average_chunk_size}\")\n",
        "\n",
        "smallest_chunk = min(chunks, key=len)\n",
        "print(f\"\\nSmallest chunk: {smallest_chunk}\")\n",
        "print(f\"\\nSmallest chunk length: {len(smallest_chunk)}\")\n",
        "\n",
        "largest_chunk = max(chunks, key=len)\n",
        "print(f\"\\nLargest chunk: \\n{textwrap.fill(largest_chunk, width=100)}\")\n",
        "print(f\"\\nLargest chunk length: {len(largest_chunk)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7I05ySb65Tb"
      },
      "source": [
        "Hopefully, your implementation solved the problem of unrelated sentences existing in the same space. What it might not have solved, however, was having some very small chunks and some very large chunks. Let's move on to see how we can use a chunker provided by LangChain. After that, there are some bonus exercises that you may wish to implement that address the chunk size issue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMI8pOzdx3Tq"
      },
      "source": [
        "# Chunking with LangChain\n",
        "\n",
        "LangChain provides us with a chunking tool that takes a similar approach to what we've just implemented ourselves. Let's see an example in action to see how it compares to our solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJLtwnpFyHB1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import textwrap\n",
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "\n",
        "text_splitter = SemanticChunker(\n",
        "    OpenAIEmbeddings(),\n",
        "    breakpoint_threshold_type=\"percentile\",\n",
        "    breakpoint_threshold_amount=50.0,\n",
        "    min_chunk_size=200,\n",
        ")\n",
        "\n",
        "chunk_documents = text_splitter.create_documents([launch_school_docs])\n",
        "chunks = [document.page_content for document in chunk_documents]\n",
        "\n",
        "print(*chunks, sep=\"\\n-----------\\n\")\n",
        "\n",
        "print('**************** Chunk data ****************')\n",
        "\n",
        "print(f\"\\nNumber of chunks: {len(chunks)}\")\n",
        "\n",
        "average_chunk_size = sum([len(chunk) for chunk in chunks]) / len(chunks)\n",
        "print(f\"\\nAverage chunk size: {average_chunk_size}\")\n",
        "\n",
        "smallest_chunk = min(chunks, key=len)\n",
        "print(f\"\\nSmallest chunk: {smallest_chunk}\")\n",
        "print(f\"\\nSmallest chunk length: {len(smallest_chunk)}\")\n",
        "\n",
        "largest_chunk = max(chunks, key=len)\n",
        "print(f\"\\nLargest chunk: \\n{textwrap.fill(largest_chunk, width=100)}\")\n",
        "print(f\"\\nLargest chunk length: {len(largest_chunk)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXthnIlJy8V3"
      },
      "source": [
        "Experiment with the `breakpoint_threshold_amount` to see the different chunks. You can also take a look at [the documentation](https://python.langchain.com/api_reference/experimental/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html) to see the other parameters we can use to control the chunking.\n",
        "\n",
        "## Bonus Features\n",
        "\n",
        "These bonus features are for your custom chunking implementation that you made above.\n",
        "\n",
        "- **Managing Chunk Size**: Add a `min_chunk_size` and `max_chunk_size` keyword argument that allows us to set some limits. You might consider a two-step chunking process. Does it make sense to cut of a chunk as soon as it reaches it's maximum, or revisit the largest chunks later and split them evenly?\n",
        "\n",
        "- **Overlapping Chunks**: Try implementing a sliding-window technique such that the end of one chunk is included in the beginning of the next chunk. This can help to avoid context loss between chunks.\n",
        "\n",
        "- **Token Based Chunking**: Use `tiktoken` to base your chunk size on token count rather than number of characters."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
